{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nPX0Etiwz5w"
   },
   "source": [
    "[![Open in COlab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1w0la4sy3fByhBEiYewQqURTRNKBWI5-f#scrollTo=6nPX0Etiwz5w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htsPHnmVoG4v"
   },
   "source": [
    "# Prediction of Product Sales\n",
    "\n",
    "Jamison Hunter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ACtOhFlozSW"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHVBpYX6Xbhr"
   },
   "source": [
    "The purpose of this notebook is to analyze outlet product sales in order to utilize a machine learning algorithm to predict future sales. Along the way, I will go through the process of cleaning the data, exploring the data, and pointing out any relevant observations I may come across."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQVwzZTBJgqX"
   },
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvmlkZfkDlUm"
   },
   "source": [
    "Variables and Descriptions\n",
    "* Item_Identifier\t- Unique product ID\n",
    "* Item_Weight\t- Weight of product\n",
    "* Item_Fat_Content - Whether the product is low fat or regular\n",
    "* Item_Visibility\t- The percentage of total display area of all products in a store allocated to the particular product\n",
    "* Item_Type\t- The category to which the product belongs\n",
    "* Item_MRP - Maximum Retail Price (list price) of the product\n",
    "* Outlet_Identifier\t- Unique store ID\n",
    "* Outlet_Establishment_Year\t- The year in which store was established\n",
    "* Outlet_Size\t- The size of the store in terms of ground area covered\n",
    "* Outlet_Location_Type\t- The type of area in which the store is located\n",
    "* Outlet_Type\t- Whether the outlet is a grocery store or some sort of supermarket\n",
    "* Item_Outlet_Sales\t- Sales of the product in the particular store. This is the target variable to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XenrR1-_o3S7"
   },
   "source": [
    "# Load and Inspect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-SGlD2nC6Uq"
   },
   "source": [
    "# Loading Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "67YPS1f_pfHV"
   },
   "outputs": [],
   "source": [
    "# loading in the necessary imports for data analysis and cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# data processing importsfrom\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer,make_column_selector\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# imports necessary for predictive models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "# neural network imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score\n",
    "from tensorflow.keras import metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uao8uZp6C0we",
    "outputId": "cb0e7fa5-3a15-4f55-e921-b04211295afd"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# giving the notebook permission to access my Google drive in order to access sales_predictions.csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# giving the notebook permission to access my Google drive in order to access sales_predictions.csv\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARHd_D--EN5p"
   },
   "source": [
    "# Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCCAPPtLDDQL"
   },
   "outputs": [],
   "source": [
    "# saving the Google drive path for sales_predictions.csv as a variable\n",
    "sales_file = \"/content/drive/MyDrive/sales_predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "aXIjK352DVza",
    "outputId": "9634bd99-f369-4d69-f78c-a36f877a522e"
   },
   "outputs": [],
   "source": [
    "# turning the sales_file variable into a data frame\n",
    "df = pd.read_csv(sales_file)\n",
    "# creating a copy for a future portion of the project\n",
    "df2 = df.copy()\n",
    "# printing data frame information along with the first 5 rows of the data frame\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjpmvFrTpFg9"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBZIMdJBjt79"
   },
   "source": [
    "In order to present consistent charts throughout, I will begin by setting up the default graph style for this data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYY0P847kIv9"
   },
   "outputs": [],
   "source": [
    "# setting the style for seaborn graphs\n",
    "plt.style.use(['dark_background','seaborn-muted', 'seaborn-poster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlqAcX_jDc5W",
    "outputId": "0a7c1a57-4947-4fa1-8b87-a684a09c9ac8"
   },
   "outputs": [],
   "source": [
    "# stating the number of rows and columns in the data set\n",
    "print(f\"This data set has {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhVA0GDSDtFW",
    "outputId": "d7a79f1a-0599-4b5c-cfdf-6cbef1355a34"
   },
   "outputs": [],
   "source": [
    "# checking data types in the data set\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1Sw3xTUD1I6",
    "outputId": "ae46119b-e11b-40fb-e288-c2da3fa7a68f"
   },
   "outputs": [],
   "source": [
    "# checking for duplicate rows\n",
    "print(f'There are {df.duplicated().sum()} duplicate rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vv9OAQzBCX-W"
   },
   "source": [
    "# Addressing Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p92UqweAKc0N",
    "outputId": "f0792e68-9bb4-412f-93fb-83f64a12e842"
   },
   "outputs": [],
   "source": [
    "# stating the total number of missing values in the data set\n",
    "print(f\"There are {df.isna().sum().sum()} missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWWyn6mzD_xn",
    "outputId": "6de28628-6211-4df6-d797-6f32ed73d01d"
   },
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpxcZFi8HeEI",
    "outputId": "2f7295ed-06ab-4052-8d8c-e208b14c5321"
   },
   "outputs": [],
   "source": [
    "# displaying the percentage of missing data by column\n",
    "print(df.isna().sum()/len(df)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6V3p09uEIit"
   },
   "source": [
    "Numerous values seem to be missing from both Item_Weight and Outlet_Size.\n",
    "\n",
    "\n",
    "\n",
    "*   Item_Weight is missing 17.165%.\n",
    "*   Outlet_Size is missing 28.276%.\n",
    "\n",
    "These missing values should show up as \"NaN\" values, which will need to be replaced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "JBqctHdYTG0x",
    "outputId": "fe111c85-c654-4601-e326-325fb4a7b0d4"
   },
   "outputs": [],
   "source": [
    "# inspecting the first 5 rows of missing data in Item_Weight\n",
    "df[df[\"Item_Weight\"].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "_h9iGr3UTlKd",
    "outputId": "721f5c09-4e0d-4140-df44-51881596663a"
   },
   "outputs": [],
   "source": [
    "# inspecting the first 5 rows of missing data in Outlet_Size\n",
    "df[df[\"Outlet_Size\"].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5k_LWBlXM-oM"
   },
   "source": [
    "Since the amount of values missing from both Item_Weight and Outlet_Size is less than 50% and greater than 5%, I will choose to replace any \"NaN\" values with \"Unknown\" in the Outlet_Size column. In order to better decide what should be done concerning Item_Weight, I will create a histogram in order to gather if I should use mean or median as the replacement value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "id": "ZoLQw_i21L5b",
    "outputId": "72156c20-585f-4f90-8a6f-59179a4dba77"
   },
   "outputs": [],
   "source": [
    "ax = df[\"Item_Weight\"].hist(bins = 30, edgecolor = \"black\")\n",
    "ax.set_title(\"Item Weight Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaW4LtNy1mXs"
   },
   "source": [
    "Based on the results of this histogram, I believe it would be best to replace any unknows in Item_Weight with the mean value. This is because the distribution of Item_Weight doesn't skew too far to the left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xULlisAANdS7",
    "outputId": "20b7f949-a8ac-4e0e-c477-a57021e714dd"
   },
   "outputs": [],
   "source": [
    "i_mean = np.mean(df[\"Item_Weight\"])\n",
    "# replacing missing values with Item_Weight average\n",
    "df['Item_Weight'].fillna(value = i_mean, inplace = True)\n",
    "# filling in missing data in the Outlet_Size solumn\n",
    "df['Outlet_Size'].fillna(value = \"Unknown\", inplace = True)\n",
    "# double checking to see if the missing data has been filled in Outlet_Size\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beLZ91yIOm3s"
   },
   "source": [
    "The data set no longer has any missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dy4rfePW-lM"
   },
   "source": [
    "# Addressing Inconsistent Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u6heRpPXZ6Z"
   },
   "source": [
    "I am going to check every column taking object class data in order to make sure the values are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtY2kU-FWrR8",
    "outputId": "d7d504b6-69b0-4a55-d787-2e186b65f3b0"
   },
   "outputs": [],
   "source": [
    "# checking each object column's unique entries\n",
    "# adding spacing for ease of reading\n",
    "print(df[\"Item_Fat_Content\"].unique())\n",
    "print()\n",
    "print(df[\"Item_Type\"].unique())\n",
    "print()\n",
    "print(df[\"Outlet_Location_Type\"].unique())\n",
    "print()\n",
    "print(df[\"Outlet_Type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehB-TWfOY2js"
   },
   "source": [
    "There appears to be several inconsistencies in the Item_Fat_Content column. I will address this by changing \"LF\" and \"low fat\" into \"Low Fat.\" I will also change any \"reg\" values into \"Regular.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gXd-dWBzaG-l",
    "outputId": "7f3bb6a4-0d2c-47ef-825e-2149c54fe90c"
   },
   "outputs": [],
   "source": [
    "# replacing LF with Low Fat\n",
    "df = df.replace(to_replace=\"LF\",\n",
    "           value=\"Low Fat\")\n",
    "# replacing low fat with Low Fat\n",
    "df = df.replace(to_replace=\"low fat\",\n",
    "           value=\"Low Fat\")\n",
    "# replacing reg with Regular\n",
    "df = df.replace(to_replace=\"reg\",\n",
    "           value=\"Regular\")\n",
    "# checking to make sure the values of Item_Fat_Content are correct\n",
    "print(df[\"Item_Fat_Content\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mlx4x9dpca4l"
   },
   "source": [
    "# Numerical Column Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWVMLEH8c2l0"
   },
   "source": [
    "In this section, there will be listed the necessary statistics for numerical columns in order to find the mean, median, and mode, of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Ty6sbSdfdFid",
    "outputId": "c522267b-3fd4-4d5a-b660-63abf5226d7c"
   },
   "outputs": [],
   "source": [
    "# stating the statistics for each numerical column\n",
    "df.describe(include = \"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 866
    },
    "id": "Fxds2acRlJmp",
    "outputId": "b0361181-e02a-4cee-adec-3c1a8f8e419a"
   },
   "outputs": [],
   "source": [
    "# setting the numerical column statistics equal to a variable\n",
    "ndata = df.describe(include = \"number\")\n",
    "# generating numerical column statistics as a bar graph\n",
    "ax = ndata.plot(kind = \"box\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "# setting the graph title\n",
    "ax.set_title(\"Numerical Column Deviation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0yAUCA9y-Q1"
   },
   "source": [
    "Based on the above graph, it can be seen that Item_Outlet_Sales can vary quite wildly at its minimum and maximum points. This will be useful information as analysis continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQvkTtcc0Zdp"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBePLCtyhrAa"
   },
   "source": [
    "In this next portion, the data set will be analyzed graphically in order to search for possible correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "TkCzk7ve0egU",
    "outputId": "87c27e88-3668-4dd0-b711-bee5f09ff857"
   },
   "outputs": [],
   "source": [
    "# checking for correlations numerically\n",
    "corr = df.corr()\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "id": "Yf40zFqDjFBD",
    "outputId": "0ab5b395-6ecb-4fc5-e315-61267daad874"
   },
   "outputs": [],
   "source": [
    "# generating a heatmap of all numerical column data correlations\n",
    "ax = sns.heatmap(corr, cmap = \"Blues\", annot = True)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "ax.set_title(\"Numerical Data Correlations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKMm-DtFjV8e"
   },
   "source": [
    "There seems to be a moderate correlation between Item_MRP and Outlet_Sales. There even seems to be an ever so slight negative correlation between Item_Visibility and Item_Outlet_Sales, which I find somewhat surprising upon first glance. Though, this could be due to larger items having greater prices and more inconvenience involved in moving large items. However, this is purely speculation. The negative correlation is already so small that it can hardly be considered meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Yq6JHCFCO4"
   },
   "source": [
    "Next, I will take a look at some of the object data columns in order to check for any patterns, which can be analyzed further during the exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "HzBDmtWBFX4w",
    "outputId": "61dcbffb-ebd9-43bd-c669-a7ada3ddcdaf"
   },
   "outputs": [],
   "source": [
    "# checking object data type columns in order to see what should be inspected\n",
    "df.describe(include = \"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "id": "9VP479isFuBh",
    "outputId": "282939ce-ebc6-4546-871c-10604ff9e763"
   },
   "outputs": [],
   "source": [
    "# generating bar graph of Item_Fat_Content value counts\n",
    "ax = df[\"Item_Fat_Content\"].value_counts().plot(kind = \"barh\")\n",
    "ax.set_title(\"Item Fat Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oUVPljSGIt0"
   },
   "source": [
    "The majority of items are low fat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "id": "HHmn-ipEGPfd",
    "outputId": "241db676-9a4b-4735-80dc-d2d4ed7eeb12"
   },
   "outputs": [],
   "source": [
    "# generating bar graph of Item_Type\n",
    "ax = df[\"Item_Type\"].value_counts().plot(kind = \"barh\")\n",
    "ax.set_title(\"Item Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nav4WFAmGxb6"
   },
   "source": [
    "Fruits and Vegetables represent the largest category while Seafood represents the smallest category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "7HrDYRPNHDtv",
    "outputId": "6c6d59f2-f356-4bac-932a-37be56fd2aed"
   },
   "outputs": [],
   "source": [
    "# generating a bar graph of outlet location types\n",
    "ax = df[\"Outlet_Location_Type\"].value_counts().plot(kind = \"barh\")\n",
    "ax.set_title(\"Outlet Location Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "dAaHTDfTHcQK",
    "outputId": "3a7527af-a10c-4ada-a7db-86e7340509f9"
   },
   "outputs": [],
   "source": [
    "# generating a bar graph of outlet types\n",
    "ax = df[\"Outlet_Type\"].value_counts().plot(kind = \"barh\")\n",
    "ax.set_title(\"Outlet Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma5mo4UXHqFP"
   },
   "source": [
    "Supermarket Type 1 is by far the most common outlet type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "psq-3t9DWoky",
    "outputId": "3793b3ed-7035-4351-a86d-e63678bc1b88"
   },
   "outputs": [],
   "source": [
    "# generating a bar graph of Outlet_Size\n",
    "ax = df[\"Outlet_Size\"].value_counts().plot(kind = \"barh\")\n",
    "ax.set_title(\"Outlet Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLIkYYGRpMmL"
   },
   "source": [
    "# Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYxmFDKSAFN3"
   },
   "source": [
    "One of the first areas of exploration will be the relationship between an item's fat content and outlet sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "FVVpUAumRazn",
    "outputId": "546e1537-8ff4-4f6e-a151-71e59c0a0196"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# establishing fat content data frames\n",
    "df_fat = df[\"Item_Fat_Content\"] == \"Low Fat\"\n",
    "df_reg = df[\"Item_Fat_Content\"] == \"Regular\"\n",
    "\n",
    "# gaining the outlet sales rates based on item fat content\n",
    "rate_fat = df[df_fat][\"Item_Outlet_Sales\"]\n",
    "rate_reg = df[df_reg][\"Item_Outlet_Sales\"]\n",
    "\n",
    "# generating a histogram with both data frames plotted\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plt.hist([rate_fat, rate_reg], bins = 10, edgecolor = \"black\")\n",
    "ax.set_title(\"Correlation Between Fat Content & Outlet Sales\")\n",
    "ax.set_xlabel(\"Outlet Sales\")\n",
    "ax.set_ylabel(\"Item Count\")\n",
    "\n",
    "#generating a legend for the graph\n",
    "ax.legend(labels = [\"Low Fat\", \"Regular\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "aZv1I2opfTLI",
    "outputId": "8fe8aa68-5090-4801-9239-7f418bc5521e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.barplot(x='Item_Fat_Content',y='Item_Outlet_Sales',data=df)\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_xlabel(\"Fat Content\")\n",
    "ax.set_title(\"Sales By Item Fat Content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK4FSIA7qVFm"
   },
   "source": [
    "Based on this graph, it is apparent that there are more low fat items than regular fat items in general. However, it seems that regular items have a tendancy to have higher outlet sales. Though, this looks to be only a slight difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMNI2PqhxNLI"
   },
   "source": [
    "Next, we will take a look at the distribution of Item_Visibility. As a reminder, the Item_Visibility column represents the percentage of display space taken up within the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "Idw3oYKwwj2h",
    "outputId": "60c7234c-58ba-4a2a-be7a-9ba268ad8483"
   },
   "outputs": [],
   "source": [
    "# generating a histogram of Item_Visibility\n",
    "ax = df[\"Item_Visibility\"].hist(bins = 30, edgecolor = \"black\")\n",
    "ax.set_title(\"Item Visibility Distribution\")\n",
    "ax.set_xlabel(\"Item Visibility\")\n",
    "ax.set_ylabel(\"Item Count\")\n",
    "\n",
    "# acquiring the mean and average in order to place them onto a legend\n",
    "mean_vis = df['Item_Visibility'].mean()\n",
    "ax.axvline(mean_vis,color='green', ls=':',\n",
    "           label=f\"Mean Visibility = {mean_vis:,.2f}\",);\n",
    "\n",
    "med_vis = df['Item_Visibility'].median()\n",
    "ax.axvline(med_vis,color='blue', ls=':',\n",
    "           label=f\"Median Visibility = {med_vis:,.2f}\");\n",
    "\n",
    "\n",
    "ax.set_xlabel(ax.xaxis.get_label().get_text(),\n",
    "              fontsize='x-large')\n",
    "ax.set_ylabel(ax.yaxis.get_label().get_text(),\n",
    "              fontsize='x-large')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4JxlPghx2eg"
   },
   "source": [
    "This demonstrates that most items take up a very small percentage of allocated display space, which makes sense. Most items certainly trend below 10% of the store's total display space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "qRO70OdTZxNk",
    "outputId": "332ea03a-7068-4beb-8499-53c8956a7c20"
   },
   "outputs": [],
   "source": [
    "# creating a visualization in order to compare Item_MRP to Outlet_Sales by Outlet_Size\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.lineplot(\n",
    "    x=\"Item_MRP\",\n",
    "    y=\"Item_Outlet_Sales\",\n",
    "    data=df,\n",
    "    palette='bright',\n",
    "    hue='Outlet_Size');\n",
    "ax.set_xlabel(\"Item MSRP\")\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_title(\"Item MSRP & Sales By Outlet Size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8ORunb7a3jz"
   },
   "source": [
    "Outlet sales are generally the highest at medium sized outlets based on the above graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "GdJ1qGnSbZAp",
    "outputId": "0252796b-08a1-44e8-ea54-b0e97ac71ead"
   },
   "outputs": [],
   "source": [
    "# creating a visualization in order to compare Item_MRP to Outlet_Sales by Outlet_Size\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.lineplot(\n",
    "    x=\"Item_MRP\",\n",
    "    y=\"Item_Outlet_Sales\",\n",
    "    data=df,\n",
    "    palette='bright',\n",
    "    hue='Outlet_Type');\n",
    "ax.set_xlabel(\"Item MSRP\")\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_title(\"Item MSRP & Sales By Outlet Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MN9JQAJibHSp"
   },
   "source": [
    "Based on the above graph, grocery stores have some of the highest outlet sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "HgIWU61echCc",
    "outputId": "af5928bb-32c9-47d9-fe37-39ba5d6eaa1a"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.barplot(x='Outlet_Type',y='Item_Outlet_Sales',data=df)\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_xlabel(\"Outlet Type\")\n",
    "ax.set_title(\"Sales By Outlet Type\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "iKXoH6A1dU43",
    "outputId": "a0ab55fa-ed19-4329-c44f-a1b44e1679d7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.barplot(x='Outlet_Size',y='Item_Outlet_Sales',data=df)\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_xlabel(\"Outlet Size\")\n",
    "ax.set_title(\"Sales By Outlet Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "NzaLQEE1eNGT",
    "outputId": "4c777cd0-044f-458c-e660-76a53bc53a3d"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.barplot(x='Item_Type',y='Item_Outlet_Sales',data=df)\n",
    "ax.set_ylabel(\"Outlet Sales\")\n",
    "ax.set_xlabel(\"Item Type\")\n",
    "ax.set_title(\"Sales By Item Type\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 740
    },
    "id": "hpyhyY-Ef0IS",
    "outputId": "b054fe22-fcae-4cf0-8e84-56ca077df61b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = sns.barplot(x='Item_Type',y='Item_Visibility',data=df)\n",
    "ax.set_xlabel(\"Item Type\")\n",
    "ax.set_ylabel(\"Visibility\")\n",
    "ax.set_title(\"Item Visibility By Item Type\")\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqaDIVTlkce5"
   },
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYSM1AOUkj11"
   },
   "source": [
    "The purpose of this next portion is to prepare the data for a machine learning algorithm and to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "5JdYbaQAldQy",
    "outputId": "2b2a05b5-c05d-409f-91b5-c6a02d80d2e6"
   },
   "outputs": [],
   "source": [
    "# checking to see if df2 has unedited data from the previous sections\n",
    "df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMr0B5HmlzU7"
   },
   "source": [
    "This confirms that df2 is our original starting data frame before any data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmUdphsk2_up"
   },
   "source": [
    "For the purposes of constructing a predictive model, the Item_Identifier column will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOEins5e3S1P"
   },
   "outputs": [],
   "source": [
    "# removing the Item_Identifier column\n",
    "df2.drop(columns = [\"Item_Identifier\"], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFSw9KJ8NpuU"
   },
   "source": [
    "# Fixing Catagorical Data Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdpiNDMmNv2O",
    "outputId": "c49974e7-763a-46ba-e081-363e21e3fca6"
   },
   "outputs": [],
   "source": [
    "# checking each object column's unique entries\n",
    "# adding spacing for ease of reading\n",
    "print(df2[\"Item_Fat_Content\"].unique())\n",
    "print()\n",
    "print(df2[\"Item_Type\"].unique())\n",
    "print()\n",
    "print(df2[\"Outlet_Location_Type\"].unique())\n",
    "print()\n",
    "print(df2[\"Outlet_Type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DS9zoJBOJP5",
    "outputId": "7b22b1d2-a1a2-4be3-b884-3e186196a8ac"
   },
   "outputs": [],
   "source": [
    "# replacing LF with Low Fat\n",
    "df2 = df2.replace(to_replace=\"LF\",\n",
    "           value=\"Low Fat\")\n",
    "# replacing low fat with Low Fat\n",
    "df2 = df2.replace(to_replace=\"low fat\",\n",
    "           value=\"Low Fat\")\n",
    "# replacing reg with Regular\n",
    "df2 = df2.replace(to_replace=\"reg\",\n",
    "           value=\"Regular\")\n",
    "# checking to make sure the values of Item_Fat_Content are correct\n",
    "print(df2[\"Item_Fat_Content\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B5FF3yxQ6tZ",
    "outputId": "4afc5d1a-6736-432f-912b-95baf257af10"
   },
   "outputs": [],
   "source": [
    "df2[\"Outlet_Establishment_Year\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c7xzJnGOeNh"
   },
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XKXNOyVOjoT"
   },
   "source": [
    "Item_Outlet_Sales will be the target data set and will thus be our y value while every other necessary variable will be within the X data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cr73oWibO1Vs"
   },
   "outputs": [],
   "source": [
    "# performing data split\n",
    "y = df2[\"Item_Outlet_Sales\"]\n",
    "X = df2.drop(columns = [\"Item_Outlet_Sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZo4sYMsPT7n"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5A5TRxcR-Ky"
   },
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "id": "iiAsQT9JTJOB",
    "outputId": "7f47b507-8993-4c75-c43b-7a96d3edd066"
   },
   "outputs": [],
   "source": [
    "# Instantiate the transformers\n",
    "scaler = StandardScaler()\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "# Prepare separate processing pipelines for numeric and categorical data\n",
    "num_pipe = make_pipeline(mean_imputer, scaler)\n",
    "cat_pipe = make_pipeline(freq_imputer, ohe)\n",
    "# Create ColumnSelectors for the the numeric and categorical data\n",
    "cat_selector = make_column_selector(dtype_include='object')\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "# Combine the Pipelines and ColumnSelectors into tuples for the ColumnTransformer\n",
    "cat_tuple = (cat_pipe, cat_selector)\n",
    "num_tuple = (num_pipe, num_selector)\n",
    "# Create the preprocessing ColumnTransformer\n",
    "preprocessor = make_column_transformer(cat_tuple, num_tuple, remainder='drop')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF1L_gQT0C7a"
   },
   "outputs": [],
   "source": [
    "# process the data\n",
    "preprocessor.fit(X_train)\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-vdsltPgBw7"
   },
   "source": [
    "# Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdtUgp4lhnbQ"
   },
   "outputs": [],
   "source": [
    "# instantiating the linear regression model\n",
    "linreg = LinearRegression()\n",
    "# combining the processor with the linear regression\n",
    "linreg_pipe = make_pipeline(preprocessor, linreg)\n",
    "# fitting the linear regression pipeline onto the training data\n",
    "linreg_pipe.fit(X_train, y_train)\n",
    "# predictions\n",
    "linreg_train_preds = linreg_pipe.predict(X_train)\n",
    "linreg_test_preds = linreg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YldU4WxBbxWo",
    "outputId": "7aee3eb8-43b8-4529-b391-a4e01da02501"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(linreg_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(linreg_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(linreg_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(linreg_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, linreg_train_preds)\n",
    "test_r2 = r2_score(y_test, linreg_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcpY_woXfVs-"
   },
   "source": [
    "# Adjusted Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VMBDdMGfAbN",
    "outputId": "5e862ed8-b222-45d4-f2c4-705006f5ae6d"
   },
   "outputs": [],
   "source": [
    "# getting linear regression model parameters\n",
    "linreg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3aSLRENhfiUj"
   },
   "outputs": [],
   "source": [
    "# Linear Regression parameter grid\n",
    "linreg_param_grid = {\"fit_intercept\" : [True, False],\n",
    "                  \"positive\" : [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "lILvP9yIgfSB",
    "outputId": "4de2acbd-c1d5-41dc-f7bd-bb999f484000"
   },
   "outputs": [],
   "source": [
    "# generating the best hyperparameters from the linear regression parameter grid\n",
    "base_estimator = LinearRegression()\n",
    "X, y = make_classification(n_samples=1000)\n",
    "sh = HalvingGridSearchCV(base_estimator, linreg_param_grid, cv=5,\n",
    "                         factor=2).fit(X, y)\n",
    "sh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-z5SMzMhbP3"
   },
   "outputs": [],
   "source": [
    "# instantiating the linear regression model\n",
    "adj_linreg = LinearRegression(positive = True)\n",
    "# combining the processor with the linear regression\n",
    "adj_linreg_pipe = make_pipeline(preprocessor, adj_linreg)\n",
    "# fitting the linear regression pipeline onto the training data\n",
    "adj_linreg_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "adj_linreg_train_preds = adj_linreg_pipe.predict(X_train)\n",
    "adj_linreg_test_preds = adj_linreg_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3MXXsCTiXz-",
    "outputId": "bb5a2b2a-d358-4e43-fc0a-d725c1d09bd9"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(adj_linreg_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(adj_linreg_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(adj_linreg_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(adj_linreg_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, adj_linreg_train_preds)\n",
    "test_r2 = r2_score(y_test, adj_linreg_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZFoh4N5ihuK"
   },
   "source": [
    "Adjusting the linear regression model by setting positive equal to true improved the model only marginally. Overall, the linear regression model seems to be unable to make consistently accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xx7FJ6z2gGIs"
   },
   "source": [
    "# Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOVAy3c7pq8l"
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit gradient boosting classifier\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "gbr_pipe = make_pipeline(preprocessor, gbr)\n",
    "\n",
    "gbr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "gbr_train_preds = gbr_pipe.predict(X_train)\n",
    "gbr_test_preds = gbr_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLG1B7iUtZj2",
    "outputId": "195f5e93-2e9f-4435-97ec-effd92c99379"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(gbr_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(gbr_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(gbr_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(gbr_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, gbr_train_preds)\n",
    "test_r2 = r2_score(y_test, gbr_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQZisPz-l_mt"
   },
   "source": [
    "# Adjusted Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YskTkSAfmFtu",
    "outputId": "7173841e-6292-41d0-c875-378c9b5c5573"
   },
   "outputs": [],
   "source": [
    "# checking the gradient boosting hyperparameters\n",
    "gbr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE4yAxwbmr6F"
   },
   "outputs": [],
   "source": [
    "# Linear Regression parameter grid\n",
    "gbr_param_grid = {\"max_depth\" : [10, 80, 100, 110, None],\n",
    "                 'min_samples_leaf': [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "BDRCLOxOnbI_",
    "outputId": "f2cf1ce7-b126-4804-cb78-2a37a45251fc"
   },
   "outputs": [],
   "source": [
    "# generating the best hyperparameters from the gradient boosting parameter grid\n",
    "base_estimator = GradientBoostingRegressor()\n",
    "X, y = make_classification(n_samples=1000)\n",
    "sh = HalvingGridSearchCV(base_estimator, gbr_param_grid, cv=5,\n",
    "                         factor=2).fit(X, y)\n",
    "sh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HfREE4-MpRnk"
   },
   "outputs": [],
   "source": [
    "# Instantiate and fit gradient boosting classifier\n",
    "adj_gbr = GradientBoostingRegressor(max_depth = 10, min_samples_leaf = 2)\n",
    "\n",
    "adj_gbr_pipe = make_pipeline(preprocessor, adj_gbr)\n",
    "\n",
    "adj_gbr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# predictions\n",
    "adj_gbr_train_preds = adj_gbr_pipe.predict(X_train)\n",
    "adj_gbr_test_preds = adj_gbr_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czOSwj07p2mw",
    "outputId": "724dc89d-8a43-4de9-97bf-ad635e81d41b"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(adj_gbr_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(adj_gbr_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(adj_gbr_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(adj_gbr_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, adj_gbr_train_preds)\n",
    "test_r2 = r2_score(y_test, adj_gbr_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgu9Wa9sVOpF"
   },
   "source": [
    "The gradient boosting regressor obtained significantly better training results when adjusted but still seems to suffer in its overall predictive capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv1U5SD2z_SL"
   },
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU1VznAH0Di0"
   },
   "outputs": [],
   "source": [
    "# creating and testing a random forest model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# creating a pipeline using the scaler and preprocessor\n",
    "rf_pipe = make_pipeline(preprocessor, rf)\n",
    "\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "rf_pipe.predict(X_test)\n",
    "\n",
    "# predictions\n",
    "rf_train_preds = rf_pipe.predict(X_train)\n",
    "rf_test_preds = rf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rf5NDg-XsHn-",
    "outputId": "8d000049-3316-4b1b-d732-6fe7456db4b2"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(rf_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(rf_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(rf_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(rf_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, rf_train_preds)\n",
    "test_r2 = r2_score(y_test, rf_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L240Vm45seem"
   },
   "source": [
    "# Adjusted Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_R1cePMEvBHi",
    "outputId": "ca7fe915-f9ed-4399-d5ad-83ca61b180fa"
   },
   "outputs": [],
   "source": [
    "# checking random forest parameters\n",
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-u6YLZ7vHJb"
   },
   "outputs": [],
   "source": [
    "# random forest parameter grid\n",
    "rf_param_grid = {'bootstrap': [True, False],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 \"max_depth\" : [10, 80, 100, 110, None],\n",
    "                 'min_samples_leaf': [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "H4NDPl1zwL5X",
    "outputId": "0239b990-32df-4de5-f815-3a7dfc06df35"
   },
   "outputs": [],
   "source": [
    "# generating the best hyperparameters from the random forest parameter grid\n",
    "base_estimator = RandomForestRegressor()\n",
    "X, y = make_classification(n_samples=1000)\n",
    "sh = HalvingGridSearchCV(base_estimator, rf_param_grid, cv=5,\n",
    "                         factor=2).fit(X, y)\n",
    "sh.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LPlusQ5w3jp"
   },
   "outputs": [],
   "source": [
    "# creating and testing a random forest model\n",
    "adj_rf = RandomForestRegressor(random_state = 42, max_features = \"auto\", min_samples_leaf = 4)\n",
    "# creating a pipeline using the scaler and preprocessor\n",
    "adj_rf_pipe = make_pipeline(preprocessor, adj_rf)\n",
    "\n",
    "adj_rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "adj_rf_pipe.predict(X_test)\n",
    "\n",
    "# predictions\n",
    "adj_rf_train_preds = adj_rf_pipe.predict(X_train)\n",
    "adj_rf_test_preds = adj_rf_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBAlLmoOxqNh",
    "outputId": "d3147bee-129a-402e-b0a8-10bcdab4a759"
   },
   "outputs": [],
   "source": [
    "# evaluating the model for root mean squared error\n",
    "train_RMSE = np.sqrt(np.mean(np.abs(adj_rf_train_preds - y_train)**2))\n",
    "test_RMSE = np.sqrt(np.mean(np.abs(adj_rf_test_preds - y_test)**2))\n",
    "# mean squared error\n",
    "train_MSE = np.mean(np.abs(adj_rf_train_preds - y_train)**2)\n",
    "test_MSE = np.mean(np.abs(adj_rf_test_preds - y_test)**2)\n",
    "# r2 score\n",
    "train_r2 = r2_score(y_train, adj_rf_train_preds)\n",
    "test_r2 = r2_score(y_test, adj_rf_test_preds)\n",
    "\n",
    "print(f\"Training RMsE: {train_RMSE}\")\n",
    "print(f\"Test RMSE: {test_RMSE}\")\n",
    "print()\n",
    "print(f\"Training MsE: {train_MSE}\")\n",
    "print(f\"Test MSE: {test_MSE}\")\n",
    "print()\n",
    "print(f'Training R2: {train_r2:.2f}')\n",
    "print(f'Testing R2: {test_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8r5lEzTyHLh"
   },
   "source": [
    "The test accuracy improved slightly when adjusted but the training accuracy also decreased somewhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJILT9EFx6M9"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtYokXvp0Sbo",
    "outputId": "1b76df9d-aca8-42ee-ecee-a2dd59115886"
   },
   "outputs": [],
   "source": [
    "# defining neural network\n",
    "# Save the number of features we have as our input shape\n",
    "input_shape = X_train_proc.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPwqvmZXzwY6",
    "outputId": "3982ee3e-e3c2-43bf-9a0e-10f3cf51c085"
   },
   "outputs": [],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(X_train_proc.shape[1], # How many neurons you have in your first hidden layer\n",
    "                input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "# Second hidden layer\n",
    "model.add(Dense(X_train_proc.shape[1]-10, # How many neurons you have in your second hidden layer\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "# Output layer\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "# Compile\n",
    "model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[r2_score, metrics.MeanSquaredError(),\n",
    "                           metrics.RootMeanSquaredError()], run_eagerly = True)\n",
    "# Fit the model\n",
    "history = model.fit(X_train_proc, y_train,\n",
    "                    validation_data = (X_test_proc, y_test),\n",
    "                    epochs=100, verbose = False)\n",
    "\n",
    "print(model.evaluate(X_train_proc, y_train))\n",
    "print(model.evaluate(X_test_proc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXqP_69uG9U0"
   },
   "outputs": [],
   "source": [
    "# Learning history plotting function\n",
    "def plot_history(history):\n",
    "  \"\"\"Takes a keras model learning history and plots each metric\"\"\"\n",
    "\n",
    "  metrics = history.history.keys()\n",
    "\n",
    "  for metric in metrics:\n",
    "      if not 'val' in metric:\n",
    "        plt.plot(history.history[f'{metric}'], label=f'{metric}')\n",
    "        if f'val_{metric}' in metrics:\n",
    "          plt.plot(history.history[f'val_{metric}'], label=f'val_{metric}')\n",
    "        plt.legend()\n",
    "        plt.title(f'{metric}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "j84tTajiCTSK",
    "outputId": "fc6b9693-dff3-4441-e3e3-34bffbcd1306"
   },
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v1v6RSWfG_ti",
    "outputId": "0d519144-3f88-4dbf-ecd8-5d5339156044"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFfSi4N6GdlW"
   },
   "source": [
    "# Adjusted Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rr0Te2cLGijv",
    "outputId": "34f01bf2-3b8c-436c-ba6e-74c9bff3f328"
   },
   "outputs": [],
   "source": [
    "# Sequential model\n",
    "adj_model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "adj_model.add(Dense(X_train_proc.shape[1], # How many neurons you have in your first hidden layer\n",
    "                input_dim = input_shape, # What is the shape of your input features (number of columns)\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "# Second hidden layer\n",
    "model.add(Dropout(0.2))\n",
    "adj_model.add(Dense(X_train_proc.shape[1]-20, # How many neurons you have in your second hidden layer\n",
    "                activation = 'relu')) # What activation function are you using?\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience = 2)\n",
    "\n",
    "# Output layer\n",
    "adj_model.add(Dense(1, activation = 'linear'))\n",
    "# Compile\n",
    "adj_model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=[r2_score, metrics.MeanSquaredError(),\n",
    "                           metrics.RootMeanSquaredError()], run_eagerly = True)\n",
    "# Fit the model\n",
    "history = adj_model.fit(X_train_proc, y_train,\n",
    "                    validation_data = (X_test_proc, y_test),\n",
    "                    epochs=100, verbose = False, callbacks = [early_stopping])\n",
    "\n",
    "print(adj_model.evaluate(X_train_proc, y_train))\n",
    "print(adj_model.evaluate(X_test_proc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "1RUhwhI_HL8n",
    "outputId": "3c4552b0-3194-466f-b21f-c2a89aec2cc4"
   },
   "outputs": [],
   "source": [
    "# Visualize the loss\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CAr95-SvJHsv",
    "outputId": "c7141ef8-1231-4bd4-810a-968659b24374"
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSsiC8u-t4yd"
   },
   "source": [
    "# Predictive Model Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yVFx82yPbp6"
   },
   "source": [
    "There are two models which stand out to me as having the best metrics. The unadjusted gradient boosting regressor has the best r2 score on the testing data at %60. While the unadjusted Neural Network has the lowest root mean squared error at roughly 1045.\n",
    "\n",
    "The neural network was left rather lacking in terms of its r2 score. This measured at around 57%. The gradient boosting regressor meanwhile managed to achieve a reasonably low r2 score of roughly 1054. **This is why I would select the gradient boosting regressor for deployment.** I believe further refinement of the data itself could lead to better results in terms of this model's predictive capabilities."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOnAvLEQr6U3+QcyfRR9VN0",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
